<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>model-based reinforcement learning for control | Luke M. Antonyshyn</title> <meta name="author" content="Luke M. Antonyshyn"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/lanton.github.io/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/lanton.github.io/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/lanton.github.io/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/lanton.github.io/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lanton97.github.io/lanton.github.io/projects/mbrl/"> <link rel="stylesheet" href="/lanton.github.io/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/lanton.github.io/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/lanton.github.io/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/lanton.github.io//"><span class="font-weight-bold">Luke </span>M. Antonyshyn</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/lanton.github.io/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/lanton.github.io/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/lanton.github.io/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/lanton.github.io/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/lanton.github.io/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">model-based reinforcement learning for control</h1> <p class="post-description"></p> </header> <article> <p>This project was a component of my Masters thesis. It focussed on the development of deep model-based reinforcement learning algorithms for sample efficient robot control. This component resulted in a manuscript that is currently under review for publication. The full details and the relevant code snippets can be found in my thesis. Due to some industry partenrships, the full codebase cannot be made available.</p> <p>The code was written primarily in Python. Graphs were generated using matplotlib. The deep neural networks were constructed and trained using TensrFlow. Mathematics were done using numpy. The OpenAI Gym framework was used for environments, and the custom environments were written conforming to the Gym standard. We used the stable-baselines 3 package to compare the novel algorithm to state of the art model-free algorithms. Code for testing on the Gazebo simulation and on real robots was written using C++ and Python, with the Robot Operating System.</p> <p>Inspired by the PETS algorithm, detailed in <a href="https://arxiv.org/pdf/1805.12114.pdf" rel="external nofollow noopener" target="_blank">this paper</a>, we decided to develop a novel shooting-based MBRL algorithm, called DVPMC, that can be used to find control inputs in real-time for use cases in robotics. These shooting-based algorithms work by first learning a predictive forward model of the system dynamics. At each time step after the model is learned, a series of actions is sampled from a distribution, and the resulting states are evaluated using a known reward function. The actions resulting in the states with the best rewards are then executed before the process is repeated. Due to the long prediction horizons necessary to determine the effects of a given action, these algorithms, although offering very high sample efficiencies, result in long computation times and require very high-accuracy models.</p> <p>The key difference between our algorithm and other shooting-based algorithms was the inclusion of a learned value function that estimated the relative value of any given state. This allowed us to significantly reduce the necessary prediction horizon for our forward model. This increases computational speed and reduces the error incurred by using imperfect dynamics models autoregressively. A block diagram of the DVPMC controller can be seen below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/vimpc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/vimpc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/vimpc-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/vimpc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="DVPMC Controller" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We began by testing the algorithm on the common inverted pendulum on a cart problem. This allowed for simple debugging of the algorithm and provided a common benchmark. We trained the algorithm for 500 episodes, and compared the results to SAC and TD3, two state-of-the-art model-free RL algorithms. The training curve can be seen below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/cp_curve-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/cp_curve-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/cp_curve-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/cp_curve.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Cartpole Training Curve" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We also evaluated the learned models accuracy by showing the long term prediction of the state changes against the real states. This can be seen below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/roll2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/roll2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/roll2-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/roll2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Accuracy Evaluation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Following this, we decided to test our algorithm on a robot specific problem. We used the reach-avoid game, a type of differential game. Readers can find more details on the reach-avoid game <a href="https://fling.seas.upenn.edu/~afosr/wiki/uploads/Chaserepository/Repository/General_Reach_Avoid.pdf" rel="external nofollow noopener" target="_blank">here</a>. Our agent will play the invader, trying to reach a specified location before it can be captured by the opponent. The opponent will play the optimal strategy, but the game will be initialized such that the invader can alway win if it plays optimally.</p> <p>We developed a custom environment adhering to the OpenAI Gym design pattern, also implementing the optimal strategies for the opponent player. We added differential drive robot kinematics to the player to later allow us to test the learned policies on real robots.</p> <p>We again train the agent for 500 episodes on the problem with randomized starting configurations to encourage generalization. We also compare against the SAC and TD3 algorithms. Thetraining results can be seen below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/ra_curve-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/ra_curve-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/ra_curve-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/ra_curve.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Reach-Avoid Training Curve" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/ra_win_curve-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/ra_win_curve-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/ra_win_curve-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/ra_win_curve.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Reach-Avoid Training Win Curve" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> On the left, we show the training curve for the reach-avoid game for multiple algorithms. On the right, we show the win rate for the same algorithms on the same training run. </div> <p>As we can see, DVPMC greatly outperforms the model-free algorithms in sample efficiency.</p> <p>Following this, we decided to test the learned policy on real robots. Initially, we verified the agent by testing in Gazebo. An image capture of the Gazebo simulation can be seen below.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/gazebo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/gazebo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/gazebo-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/gazebo.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Gazebo" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Finally, we set up a real robot experiment to show the robustness of the learned policy to noise found in the real world. We used Husky differential drive robots, and a Vicom system for positioning. A diagram and a photo of the experiment setup can be seen below.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/experiment_setup-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/experiment_setup-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/experiment_setup-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/experiment_setup.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Experiment Setup" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/lanton.github.io/assets/img/mbrl/husky_crop-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/lanton.github.io/assets/img/mbrl/husky_crop-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/lanton.github.io/assets/img/mbrl/husky_crop-1400.webp"></source> <img src="/lanton.github.io/assets/img/mbrl/husky_crop.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Robots" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>A full video of the experiments can be found <a href="https://youtu.be/0Q274kcfn4c" rel="external nofollow noopener" target="_blank">here</a>. Further details on the experiments and algorithms are available in my thesis, listed under the publications tab.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Luke M. Antonyshyn. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/lanton.github.io/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/lanton.github.io/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/lanton.github.io/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/lanton.github.io/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/lanton.github.io/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/lanton.github.io/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>