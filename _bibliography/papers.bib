---
---
@article{10.1145/3564696,
author = {Antonyshyn, Luke and Silveira, Jefferson and Givigi, Sidney and Marshall, Joshua},
title = {Multiple Mobile Robot Task and Motion Planning: A Survey},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3564696},
doi = {10.1145/3564696},
abstract = {With recent advances in mobile robotics, autonomous systems, and artificial intelligence, there is a growing expectation that robots are able to solve complex problems. Many of these problems require multiple robots working cooperatively in a multi-robot system. Complex tasks may also include the interconnection of task-level specifications with robot motion-level constraints. Many recent works in the literature use multiple mobile robots to solve these complex tasks by integrating task and motion planning. We survey recent contributions to the field of combined task and motion planning for multiple mobile robots by categorizing works based on their underlying problem representations, and we identify possible directions for future research. We propose a taxonomy for task and motion planning based on system capabilities, applicable to multi-robot and single-robot systems.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {213},
numpages = {35},
keywords = {Task and motion planning, mobile robotics, cooperation, motion planning, autonomous robotics, autonomous vehicles, task planning},
selected={true}
}
@phdthesis{
author={Antonyshyn,Luka},
year={2022},
title={Deep Model-Based Reinforcement Learning for Sample Efficient Predictive Control},
journal={ProQuest Dissertations and Theses},
pages={160},
note={Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2023-06-22},
abstract={Deep reinforcement learning algorithms provide a framework for learning high performance solutions to complex problems without explicit knowledge of the systems with which the algorithms are interacting. These algorithms are of particular interest for the fields of control and robotics, which traditionally rely on exact knowledge of the system dynamics and of the desired solutions. However, most state-of-the-art deep reinforcement learning algorithms require a large amount of interaction with a system before learning an effective solution, and have problems with learning solutions when we express the effectiveness of a given solution using only sparse signals given upon failure or success in solving a problem. These problems limit the applicability of deep reinforcement learning methods to real-world systems, where data is difficult to collect and problems are naturally expressed in the form of binary successes or failures. Additionally, deep reinforcement learning algorithms are difficult to analyse due to the difficult to interpret nature of artificial neural networks.In this work, we propose and evaluate a deep model-based reinforcement learning algorithm for control which is able to learn effective solutions in far fewer interactions with the environment when compared to model-free state-of-the-art algorithms, both with sparse and dense rewards. The first proposed algorithm learns a policy competitive with that of model-free algorithms with about 47% as many episodes on a dense reward inverted pendulum on a cart task, and learns a policy which matches the maximal performance of the best performing model-free algorithm in less than 6% of the number of episodes on a sparse reward reach-avoid game. We verify the functioning of the algorithm on a real-robot experiment with no fine-tuning. We propose two variants of the algorithm that use quadratic neural networks to extract analysable predictive controllers, improving the interpretability of the method and allowing us to use nonlinear optimization methods to select actions. The version of the algorithm that uses dense neural networks as an intermediate during training reduces the number of episodes needed to learn a solution to the inverted pendulum on a cart to 8.1% of the first algorithm, or 3.8% of the model-free algorithms. We also provide analysis of a known phenomenon referred to as policy oscillation that affects approximate dynamic programming algorithms, as well as one of the proposed methods.},
keywords={Control theory; Discount rates; Deep learning; Costs; Optimization; Decision making; Neural networks; Maps; Robots; Algorithms; Performance evaluation; Markov analysis; Robotics; Artificial intelligence; Operations research; Systems science; 0790:Systems science; 0771:Robotics; 0796:Operations research; 0800:Artificial intelligence},
isbn={9798371945747},
language={English},
url={https://proxy.queensu.ca/login?url=https://www-proquest-com.proxy.queensu.ca/dissertations-theses/deep-model-based-reinforcement-learning-sample/docview/2778643197/se-2},
selected={true}
}
